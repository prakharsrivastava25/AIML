{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Questions - Internal - R6 - AIML Labs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUZjPnVXGz0Z",
        "colab_type": "text"
      },
      "source": [
        "# The Iris Dataset\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMbmpriavLE9",
        "colab_type": "text"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fu8bUU__oa7h",
        "outputId": "a4b8e854-ff16-41c3-9fbe-a3817595020f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLz1Ckvfvn6D"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWrzVTLOvn6M",
        "outputId": "87bfb555-13f7-4256-a53b-d97dd34204b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uYeJgkNuXNC",
        "colab_type": "text"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcASNsewsfQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-vVQBBqg7DI",
        "colab_type": "text"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE0EDKvQhEIe",
        "colab_type": "text"
      },
      "source": [
        "### Import dataset\n",
        "- Import iris dataset\n",
        "- Import the dataset using sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOOWpD26Haq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiaSPm54MiaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = load_iris()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta8YqInTh5v5",
        "colab_type": "text"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HERt3drbhX0i",
        "colab_type": "text"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- you can get the features using .data method\n",
        "- you can get the features using .target method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cV-_qHAHyvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.data\n",
        "y = df.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nLADR0rNz7t",
        "colab_type": "code",
        "outputId": "321c2cfb-55c0-4f5c-c9bc-f619e44694c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg1A2lkUjFak",
        "colab_type": "text"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### Create train and test data\n",
        "- use train_test_split to get train and test set\n",
        "- set a random_state\n",
        "- test_size: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYKNJL85h7pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEqCAw80PAWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0KVP17Ozaix",
        "colab_type": "text"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIjqxbhWv1zv",
        "colab_type": "text"
      },
      "source": [
        "### One-hot encode the labels\n",
        "- convert class vectors (integers) to binary class matrix\n",
        "- convert labels\n",
        "- number of classes: 3\n",
        "- we are doing this to use categorical_crossentropy as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9vv-_gpyLY9",
        "outputId": "7ddf2170-42c6-4822-883d-497547fc470b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_test = to_categorical(y_test,num_classes=3)\n",
        "y_train = to_categorical(y_train, num_classes=3,)\n",
        "y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovjLyYzWkO9s"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbIFzoPNSyYo",
        "colab_type": "text"
      },
      "source": [
        "### Initialize a sequential model\n",
        "- Define a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FvSbf1UjHtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoXSFTv6T5Lt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initializing the ANN\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dGMy999vlacX"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ibK5Jxm8iL",
        "colab_type": "text"
      },
      "source": [
        "### Add a layer\n",
        "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
        "- Apply Softmax on Dense Layer outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFleLho6WU5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKrBNSRm_o9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The amount of nodes (dimensions) in hidden layer should be the average of input and output layers.\n",
        "# This adds the input layer (by specifying input dimension) AND the first hidden layer (units)\n",
        "model.add(Dense(3, activation = 'sigmoid', input_shape=(4,)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yajAEwq_WmYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add 1st hidden layer\n",
        "model.add(Dense(3, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIyPdzX7WmwI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding the output layer\n",
        "# Notice that we do not need to specify input dim. \n",
        "# we have an output of 3 node, which is the the desired dimensions of our output (stay with the bank or not)\n",
        "# We use the sigmoid because we want probability outcomes\n",
        "model.add(Dense(3, activation = 'softmax')) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4uiTH8plmNX",
        "colab_type": "text"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJL8n8vcSyYz",
        "colab_type": "text"
      },
      "source": [
        "### Compile the model\n",
        "- Use SGD as Optimizer\n",
        "- Use categorical_crossentropy as loss function\n",
        "- Use accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc_-fjIEk1ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sihIGbRll_jT"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZZCfNGlu0i",
        "colab_type": "text"
      },
      "source": [
        "### Summarize the model\n",
        "- Check model layers\n",
        "- Understand number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elER3F_4ln8n",
        "colab_type": "code",
        "outputId": "de35b0b2-9516-4cc3-dcd4-baf48124fb29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 3)                 15        \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 12        \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 12        \n",
            "=================================================================\n",
            "Total params: 39\n",
            "Trainable params: 39\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2PiP7j3Vmj4p"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWdbfFCXmCHt",
        "colab_type": "text"
      },
      "source": [
        "### Fit the model\n",
        "- Give train data as training features and labels\n",
        "- Epochs: 100\n",
        "- Give validation data as testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO1c-5tjmBVZ",
        "colab_type": "code",
        "outputId": "255c776f-38dc-4d80-b08b-6f06f543fd7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(X_train, y_train,           \n",
        "          validation_data=(X_test,y_test),\n",
        "          epochs=100,\n",
        "          batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 112 samples, validate on 38 samples\n",
            "Epoch 1/100\n",
            "112/112 [==============================] - 2s 14ms/sample - loss: 1.1046 - accuracy: 0.3304 - val_loss: 1.0678 - val_accuracy: 0.3421\n",
            "Epoch 2/100\n",
            "112/112 [==============================] - 0s 269us/sample - loss: 1.1036 - accuracy: 0.3304 - val_loss: 1.0679 - val_accuracy: 0.3421\n",
            "Epoch 3/100\n",
            "112/112 [==============================] - 0s 240us/sample - loss: 1.1027 - accuracy: 0.3304 - val_loss: 1.0680 - val_accuracy: 0.3421\n",
            "Epoch 4/100\n",
            "112/112 [==============================] - 0s 254us/sample - loss: 1.1016 - accuracy: 0.3304 - val_loss: 1.0681 - val_accuracy: 0.3421\n",
            "Epoch 5/100\n",
            "112/112 [==============================] - 0s 232us/sample - loss: 1.1010 - accuracy: 0.3304 - val_loss: 1.0681 - val_accuracy: 0.3421\n",
            "Epoch 6/100\n",
            "112/112 [==============================] - 0s 227us/sample - loss: 1.1002 - accuracy: 0.3304 - val_loss: 1.0681 - val_accuracy: 0.3421\n",
            "Epoch 7/100\n",
            "112/112 [==============================] - 0s 241us/sample - loss: 1.0994 - accuracy: 0.3304 - val_loss: 1.0683 - val_accuracy: 0.3421\n",
            "Epoch 8/100\n",
            "112/112 [==============================] - 0s 219us/sample - loss: 1.0985 - accuracy: 0.3304 - val_loss: 1.0685 - val_accuracy: 0.3421\n",
            "Epoch 9/100\n",
            "112/112 [==============================] - 0s 232us/sample - loss: 1.0974 - accuracy: 0.3304 - val_loss: 1.0687 - val_accuracy: 0.3421\n",
            "Epoch 10/100\n",
            "112/112 [==============================] - 0s 248us/sample - loss: 1.0964 - accuracy: 0.3304 - val_loss: 1.0689 - val_accuracy: 0.3421\n",
            "Epoch 11/100\n",
            "112/112 [==============================] - 0s 250us/sample - loss: 1.0956 - accuracy: 0.3304 - val_loss: 1.0689 - val_accuracy: 0.3421\n",
            "Epoch 12/100\n",
            "112/112 [==============================] - 0s 250us/sample - loss: 1.0949 - accuracy: 0.3304 - val_loss: 1.0691 - val_accuracy: 0.3421\n",
            "Epoch 13/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 1.0941 - accuracy: 0.3304 - val_loss: 1.0693 - val_accuracy: 0.3421\n",
            "Epoch 14/100\n",
            "112/112 [==============================] - 0s 238us/sample - loss: 1.0935 - accuracy: 0.3304 - val_loss: 1.0693 - val_accuracy: 0.3421\n",
            "Epoch 15/100\n",
            "112/112 [==============================] - 0s 251us/sample - loss: 1.0930 - accuracy: 0.3304 - val_loss: 1.0695 - val_accuracy: 0.3421\n",
            "Epoch 16/100\n",
            "112/112 [==============================] - 0s 267us/sample - loss: 1.0922 - accuracy: 0.3304 - val_loss: 1.0697 - val_accuracy: 0.3421\n",
            "Epoch 17/100\n",
            "112/112 [==============================] - 0s 222us/sample - loss: 1.0914 - accuracy: 0.3304 - val_loss: 1.0699 - val_accuracy: 0.3421\n",
            "Epoch 18/100\n",
            "112/112 [==============================] - 0s 247us/sample - loss: 1.0907 - accuracy: 0.3304 - val_loss: 1.0700 - val_accuracy: 0.3421\n",
            "Epoch 19/100\n",
            "112/112 [==============================] - 0s 244us/sample - loss: 1.0902 - accuracy: 0.3304 - val_loss: 1.0701 - val_accuracy: 0.3421\n",
            "Epoch 20/100\n",
            "112/112 [==============================] - 0s 276us/sample - loss: 1.0896 - accuracy: 0.3304 - val_loss: 1.0703 - val_accuracy: 0.3421\n",
            "Epoch 21/100\n",
            "112/112 [==============================] - 0s 226us/sample - loss: 1.0890 - accuracy: 0.3304 - val_loss: 1.0706 - val_accuracy: 0.3421\n",
            "Epoch 22/100\n",
            "112/112 [==============================] - 0s 261us/sample - loss: 1.0882 - accuracy: 0.3304 - val_loss: 1.0707 - val_accuracy: 0.3421\n",
            "Epoch 23/100\n",
            "112/112 [==============================] - 0s 241us/sample - loss: 1.0878 - accuracy: 0.3304 - val_loss: 1.0709 - val_accuracy: 0.3421\n",
            "Epoch 24/100\n",
            "112/112 [==============================] - 0s 228us/sample - loss: 1.0871 - accuracy: 0.3304 - val_loss: 1.0709 - val_accuracy: 0.3421\n",
            "Epoch 25/100\n",
            "112/112 [==============================] - 0s 273us/sample - loss: 1.0866 - accuracy: 0.3304 - val_loss: 1.0712 - val_accuracy: 0.3421\n",
            "Epoch 26/100\n",
            "112/112 [==============================] - 0s 244us/sample - loss: 1.0859 - accuracy: 0.3304 - val_loss: 1.0714 - val_accuracy: 0.3421\n",
            "Epoch 27/100\n",
            "112/112 [==============================] - 0s 228us/sample - loss: 1.0854 - accuracy: 0.3304 - val_loss: 1.0715 - val_accuracy: 0.3421\n",
            "Epoch 28/100\n",
            "112/112 [==============================] - 0s 244us/sample - loss: 1.0849 - accuracy: 0.3304 - val_loss: 1.0718 - val_accuracy: 0.3421\n",
            "Epoch 29/100\n",
            "112/112 [==============================] - 0s 263us/sample - loss: 1.0844 - accuracy: 0.3304 - val_loss: 1.0720 - val_accuracy: 0.3421\n",
            "Epoch 30/100\n",
            "112/112 [==============================] - 0s 272us/sample - loss: 1.0837 - accuracy: 0.3304 - val_loss: 1.0723 - val_accuracy: 0.3421\n",
            "Epoch 31/100\n",
            "112/112 [==============================] - 0s 255us/sample - loss: 1.0832 - accuracy: 0.3304 - val_loss: 1.0726 - val_accuracy: 0.3421\n",
            "Epoch 32/100\n",
            "112/112 [==============================] - 0s 260us/sample - loss: 1.0826 - accuracy: 0.3304 - val_loss: 1.0728 - val_accuracy: 0.3421\n",
            "Epoch 33/100\n",
            "112/112 [==============================] - 0s 230us/sample - loss: 1.0821 - accuracy: 0.3304 - val_loss: 1.0730 - val_accuracy: 0.3421\n",
            "Epoch 34/100\n",
            "112/112 [==============================] - 0s 231us/sample - loss: 1.0816 - accuracy: 0.3304 - val_loss: 1.0731 - val_accuracy: 0.3421\n",
            "Epoch 35/100\n",
            "112/112 [==============================] - 0s 242us/sample - loss: 1.0813 - accuracy: 0.3304 - val_loss: 1.0732 - val_accuracy: 0.3421\n",
            "Epoch 36/100\n",
            "112/112 [==============================] - 0s 345us/sample - loss: 1.0808 - accuracy: 0.3304 - val_loss: 1.0735 - val_accuracy: 0.3421\n",
            "Epoch 37/100\n",
            "112/112 [==============================] - 0s 237us/sample - loss: 1.0803 - accuracy: 0.3304 - val_loss: 1.0736 - val_accuracy: 0.3421\n",
            "Epoch 38/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 1.0798 - accuracy: 0.3304 - val_loss: 1.0738 - val_accuracy: 0.3421\n",
            "Epoch 39/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 1.0795 - accuracy: 0.3393 - val_loss: 1.0742 - val_accuracy: 0.3421\n",
            "Epoch 40/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 1.0789 - accuracy: 0.3482 - val_loss: 1.0745 - val_accuracy: 0.3684\n",
            "Epoch 41/100\n",
            "112/112 [==============================] - 0s 228us/sample - loss: 1.0785 - accuracy: 0.3661 - val_loss: 1.0748 - val_accuracy: 0.3684\n",
            "Epoch 42/100\n",
            "112/112 [==============================] - 0s 219us/sample - loss: 1.0782 - accuracy: 0.3750 - val_loss: 1.0749 - val_accuracy: 0.3947\n",
            "Epoch 43/100\n",
            "112/112 [==============================] - 0s 233us/sample - loss: 1.0777 - accuracy: 0.3750 - val_loss: 1.0751 - val_accuracy: 0.3947\n",
            "Epoch 44/100\n",
            "112/112 [==============================] - 0s 228us/sample - loss: 1.0774 - accuracy: 0.4018 - val_loss: 1.0754 - val_accuracy: 0.3947\n",
            "Epoch 45/100\n",
            "112/112 [==============================] - 0s 223us/sample - loss: 1.0771 - accuracy: 0.4375 - val_loss: 1.0754 - val_accuracy: 0.3947\n",
            "Epoch 46/100\n",
            "112/112 [==============================] - 0s 232us/sample - loss: 1.0768 - accuracy: 0.4643 - val_loss: 1.0757 - val_accuracy: 0.4211\n",
            "Epoch 47/100\n",
            "112/112 [==============================] - 0s 235us/sample - loss: 1.0764 - accuracy: 0.5000 - val_loss: 1.0758 - val_accuracy: 0.4211\n",
            "Epoch 48/100\n",
            "112/112 [==============================] - 0s 223us/sample - loss: 1.0762 - accuracy: 0.5179 - val_loss: 1.0759 - val_accuracy: 0.4474\n",
            "Epoch 49/100\n",
            "112/112 [==============================] - 0s 224us/sample - loss: 1.0758 - accuracy: 0.5446 - val_loss: 1.0759 - val_accuracy: 0.4737\n",
            "Epoch 50/100\n",
            "112/112 [==============================] - 0s 225us/sample - loss: 1.0756 - accuracy: 0.5714 - val_loss: 1.0761 - val_accuracy: 0.4737\n",
            "Epoch 51/100\n",
            "112/112 [==============================] - 0s 227us/sample - loss: 1.0754 - accuracy: 0.5982 - val_loss: 1.0763 - val_accuracy: 0.4737\n",
            "Epoch 52/100\n",
            "112/112 [==============================] - 0s 241us/sample - loss: 1.0749 - accuracy: 0.6250 - val_loss: 1.0765 - val_accuracy: 0.5000\n",
            "Epoch 53/100\n",
            "112/112 [==============================] - 0s 236us/sample - loss: 1.0747 - accuracy: 0.6339 - val_loss: 1.0767 - val_accuracy: 0.5263\n",
            "Epoch 54/100\n",
            "112/112 [==============================] - 0s 230us/sample - loss: 1.0745 - accuracy: 0.6339 - val_loss: 1.0767 - val_accuracy: 0.5789\n",
            "Epoch 55/100\n",
            "112/112 [==============================] - 0s 240us/sample - loss: 1.0742 - accuracy: 0.6518 - val_loss: 1.0770 - val_accuracy: 0.5789\n",
            "Epoch 56/100\n",
            "112/112 [==============================] - 0s 230us/sample - loss: 1.0739 - accuracy: 0.6875 - val_loss: 1.0772 - val_accuracy: 0.5789\n",
            "Epoch 57/100\n",
            "112/112 [==============================] - 0s 238us/sample - loss: 1.0736 - accuracy: 0.6964 - val_loss: 1.0774 - val_accuracy: 0.5789\n",
            "Epoch 58/100\n",
            "112/112 [==============================] - 0s 236us/sample - loss: 1.0732 - accuracy: 0.6964 - val_loss: 1.0775 - val_accuracy: 0.5789\n",
            "Epoch 59/100\n",
            "112/112 [==============================] - 0s 232us/sample - loss: 1.0731 - accuracy: 0.6964 - val_loss: 1.0775 - val_accuracy: 0.5789\n",
            "Epoch 60/100\n",
            "112/112 [==============================] - 0s 273us/sample - loss: 1.0728 - accuracy: 0.6964 - val_loss: 1.0777 - val_accuracy: 0.5789\n",
            "Epoch 61/100\n",
            "112/112 [==============================] - 0s 281us/sample - loss: 1.0726 - accuracy: 0.6964 - val_loss: 1.0777 - val_accuracy: 0.5789\n",
            "Epoch 62/100\n",
            "112/112 [==============================] - 0s 261us/sample - loss: 1.0723 - accuracy: 0.6964 - val_loss: 1.0780 - val_accuracy: 0.5789\n",
            "Epoch 63/100\n",
            "112/112 [==============================] - 0s 248us/sample - loss: 1.0721 - accuracy: 0.6964 - val_loss: 1.0781 - val_accuracy: 0.5789\n",
            "Epoch 64/100\n",
            "112/112 [==============================] - 0s 249us/sample - loss: 1.0719 - accuracy: 0.6964 - val_loss: 1.0782 - val_accuracy: 0.5789\n",
            "Epoch 65/100\n",
            "112/112 [==============================] - 0s 226us/sample - loss: 1.0716 - accuracy: 0.6964 - val_loss: 1.0784 - val_accuracy: 0.5789\n",
            "Epoch 66/100\n",
            "112/112 [==============================] - 0s 240us/sample - loss: 1.0713 - accuracy: 0.6964 - val_loss: 1.0786 - val_accuracy: 0.5789\n",
            "Epoch 67/100\n",
            "112/112 [==============================] - 0s 264us/sample - loss: 1.0711 - accuracy: 0.6964 - val_loss: 1.0787 - val_accuracy: 0.5789\n",
            "Epoch 68/100\n",
            "112/112 [==============================] - 0s 269us/sample - loss: 1.0709 - accuracy: 0.6964 - val_loss: 1.0788 - val_accuracy: 0.5789\n",
            "Epoch 69/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 1.0707 - accuracy: 0.6964 - val_loss: 1.0788 - val_accuracy: 0.5789\n",
            "Epoch 70/100\n",
            "112/112 [==============================] - 0s 225us/sample - loss: 1.0705 - accuracy: 0.6964 - val_loss: 1.0789 - val_accuracy: 0.5789\n",
            "Epoch 71/100\n",
            "112/112 [==============================] - 0s 248us/sample - loss: 1.0703 - accuracy: 0.6964 - val_loss: 1.0789 - val_accuracy: 0.5789\n",
            "Epoch 72/100\n",
            "112/112 [==============================] - 0s 319us/sample - loss: 1.0702 - accuracy: 0.6964 - val_loss: 1.0790 - val_accuracy: 0.5789\n",
            "Epoch 73/100\n",
            "112/112 [==============================] - 0s 247us/sample - loss: 1.0700 - accuracy: 0.6964 - val_loss: 1.0791 - val_accuracy: 0.5789\n",
            "Epoch 74/100\n",
            "112/112 [==============================] - 0s 238us/sample - loss: 1.0698 - accuracy: 0.6964 - val_loss: 1.0794 - val_accuracy: 0.5789\n",
            "Epoch 75/100\n",
            "112/112 [==============================] - 0s 244us/sample - loss: 1.0695 - accuracy: 0.6964 - val_loss: 1.0795 - val_accuracy: 0.5789\n",
            "Epoch 76/100\n",
            "112/112 [==============================] - 0s 232us/sample - loss: 1.0694 - accuracy: 0.6964 - val_loss: 1.0797 - val_accuracy: 0.5789\n",
            "Epoch 77/100\n",
            "112/112 [==============================] - 0s 240us/sample - loss: 1.0691 - accuracy: 0.6964 - val_loss: 1.0799 - val_accuracy: 0.5789\n",
            "Epoch 78/100\n",
            "112/112 [==============================] - 0s 243us/sample - loss: 1.0689 - accuracy: 0.6964 - val_loss: 1.0801 - val_accuracy: 0.5789\n",
            "Epoch 79/100\n",
            "112/112 [==============================] - 0s 226us/sample - loss: 1.0687 - accuracy: 0.6964 - val_loss: 1.0802 - val_accuracy: 0.5789\n",
            "Epoch 80/100\n",
            "112/112 [==============================] - 0s 238us/sample - loss: 1.0686 - accuracy: 0.6964 - val_loss: 1.0801 - val_accuracy: 0.5789\n",
            "Epoch 81/100\n",
            "112/112 [==============================] - 0s 230us/sample - loss: 1.0684 - accuracy: 0.6964 - val_loss: 1.0804 - val_accuracy: 0.5789\n",
            "Epoch 82/100\n",
            "112/112 [==============================] - 0s 247us/sample - loss: 1.0682 - accuracy: 0.6964 - val_loss: 1.0804 - val_accuracy: 0.5789\n",
            "Epoch 83/100\n",
            "112/112 [==============================] - 0s 238us/sample - loss: 1.0680 - accuracy: 0.6964 - val_loss: 1.0807 - val_accuracy: 0.5789\n",
            "Epoch 84/100\n",
            "112/112 [==============================] - 0s 255us/sample - loss: 1.0679 - accuracy: 0.6964 - val_loss: 1.0807 - val_accuracy: 0.5789\n",
            "Epoch 85/100\n",
            "112/112 [==============================] - 0s 271us/sample - loss: 1.0676 - accuracy: 0.6964 - val_loss: 1.0809 - val_accuracy: 0.5789\n",
            "Epoch 86/100\n",
            "112/112 [==============================] - 0s 248us/sample - loss: 1.0674 - accuracy: 0.6964 - val_loss: 1.0811 - val_accuracy: 0.5789\n",
            "Epoch 87/100\n",
            "112/112 [==============================] - 0s 246us/sample - loss: 1.0673 - accuracy: 0.6964 - val_loss: 1.0813 - val_accuracy: 0.5789\n",
            "Epoch 88/100\n",
            "112/112 [==============================] - 0s 233us/sample - loss: 1.0671 - accuracy: 0.6964 - val_loss: 1.0813 - val_accuracy: 0.5789\n",
            "Epoch 89/100\n",
            "112/112 [==============================] - 0s 222us/sample - loss: 1.0670 - accuracy: 0.6964 - val_loss: 1.0815 - val_accuracy: 0.5789\n",
            "Epoch 90/100\n",
            "112/112 [==============================] - 0s 254us/sample - loss: 1.0666 - accuracy: 0.6964 - val_loss: 1.0815 - val_accuracy: 0.5789\n",
            "Epoch 91/100\n",
            "112/112 [==============================] - 0s 238us/sample - loss: 1.0666 - accuracy: 0.6964 - val_loss: 1.0817 - val_accuracy: 0.5789\n",
            "Epoch 92/100\n",
            "112/112 [==============================] - 0s 228us/sample - loss: 1.0663 - accuracy: 0.6964 - val_loss: 1.0819 - val_accuracy: 0.5789\n",
            "Epoch 93/100\n",
            "112/112 [==============================] - 0s 235us/sample - loss: 1.0662 - accuracy: 0.6964 - val_loss: 1.0818 - val_accuracy: 0.5789\n",
            "Epoch 94/100\n",
            "112/112 [==============================] - 0s 236us/sample - loss: 1.0660 - accuracy: 0.6964 - val_loss: 1.0816 - val_accuracy: 0.5789\n",
            "Epoch 95/100\n",
            "112/112 [==============================] - 0s 260us/sample - loss: 1.0659 - accuracy: 0.6964 - val_loss: 1.0816 - val_accuracy: 0.5789\n",
            "Epoch 96/100\n",
            "112/112 [==============================] - 0s 245us/sample - loss: 1.0658 - accuracy: 0.6964 - val_loss: 1.0818 - val_accuracy: 0.5789\n",
            "Epoch 97/100\n",
            "112/112 [==============================] - 0s 268us/sample - loss: 1.0657 - accuracy: 0.6964 - val_loss: 1.0820 - val_accuracy: 0.5789\n",
            "Epoch 98/100\n",
            "112/112 [==============================] - 0s 269us/sample - loss: 1.0654 - accuracy: 0.6964 - val_loss: 1.0821 - val_accuracy: 0.5789\n",
            "Epoch 99/100\n",
            "112/112 [==============================] - 0s 250us/sample - loss: 1.0652 - accuracy: 0.6964 - val_loss: 1.0821 - val_accuracy: 0.5789\n",
            "Epoch 100/100\n",
            "112/112 [==============================] - 0s 233us/sample - loss: 1.0652 - accuracy: 0.6964 - val_loss: 1.0821 - val_accuracy: 0.5789\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7664532d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re9ItAR3yS3J",
        "colab_type": "text"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liw0IFf9yVqH",
        "colab_type": "text"
      },
      "source": [
        "### Make predictions\n",
        "- Predict labels on one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5sBybi6mlLl",
        "colab_type": "code",
        "outputId": "0521f611-a028-4927-f081-eb095c4e211e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred = model.predict(X_test)\n",
        "print(y_pred[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.30593687 0.30569553 0.3467099 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exbhPX66eM1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(np.expand_dims(X_test[0],axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrJP1t-4egfl",
        "colab_type": "code",
        "outputId": "c0de5b4e-9f88-4cb4-ee4f-e4de4127d92c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.30593684, 0.30569553, 0.3467099 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSUgMq3m0bG7",
        "colab_type": "text"
      },
      "source": [
        "### Compare the prediction with actual label\n",
        "- Print the same row as done in the previous step but of actual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5WbwVPyz-qQ",
        "colab_type": "code",
        "outputId": "2f9f5e3c-ee6b-4ff5-bc2e-1bec959fee3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"Prediction: {}\".format(tf.argmax(y_pred[0])))\n",
        "print(\"Labels: {}\".format(tf.argmax(y_test[0])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction: 2\n",
            "Labels: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6H90urthfR8",
        "colab_type": "code",
        "outputId": "0c55818f-94da-4e6c-e24c-1395806ed278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_test[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 1.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrTKwbgE7NFT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1UBYPNp5Tn1",
        "colab_type": "text"
      },
      "source": [
        "# Stock prices dataset\n",
        "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
        "\n",
        "## Description\n",
        "A brief description of columns.\n",
        "- open: The opening market price of the equity symbol on the date\n",
        "- high: The highest market price of the equity symbol on the date\n",
        "- low: The lowest recorded market price of the equity symbol on the date\n",
        "- close: The closing recorded price of the equity symbol on the date\n",
        "- symbol: Symbol of the listed company\n",
        "- volume: Total traded volume of the equity symbol on the date\n",
        "- date: Date of record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctH_ZW5g-M3g"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vQbdODpH-M3r",
        "outputId": "697e082c-9d7c-4c73-90e1-55190607e511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nFQWH1tj-M38"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ho5n-xhd-M3_",
        "outputId": "effe6a53-a704-4659-d553-905896c40a77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgkl0qu6-M4F"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKgTyuA3-M4G",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_88voqAH-O6J",
        "colab_type": "text"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRHCeJqP-evf",
        "colab_type": "text"
      },
      "source": [
        "### Load the data\n",
        "- load the csv file and read it using pandas\n",
        "- file name is prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gDC6cSW_FSK",
        "colab_type": "code",
        "outputId": "b26ef0c0-d6dd-4fd4-bbfb-9da0115f40b9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 58
        }
      },
      "source": [
        "# run this cell to upload file if you are using google colab\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aff1c1a9-97ea-42d4-baca-166e06d046f6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-aff1c1a9-97ea-42d4-baca-166e06d046f6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrS7_UFXpnGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4uiVuFgtcA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stock = pd.read_csv('prices.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0bND_EJtiic",
        "colab_type": "code",
        "outputId": "c4f81405-2bdd-4a91-f8ec-98c3c2263b49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "stock.head()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlLKVPVH_BCT",
        "colab_type": "text"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J4BlzVA_gZd",
        "colab_type": "text"
      },
      "source": [
        "### Drop columnns\n",
        "- drop \"date\" and \"symbol\" column from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKEK8aEE_Csx",
        "colab_type": "code",
        "outputId": "4912bfe7-60a5-404b-d811-89696c1be172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "stock.drop(['date','symbol'], axis =1,  inplace=True)\n",
        "stock.head()"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPhO6v-AiZt",
        "colab_type": "text"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsZXmF3NAkna",
        "colab_type": "text"
      },
      "source": [
        "### Take initial rows\n",
        "- Take first 1000 rows from the data\n",
        "- This step is done to make the execution faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKs04iIHAjxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stock = stock.iloc[0:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kre7hmspuwrd",
        "colab_type": "code",
        "outputId": "573d162b-0708-491a-9fb2-d6a43abe5632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stock.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGtnapgBIJm",
        "colab_type": "text"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8u_jlbABTip"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
        "- Take \"volume\" column as label\n",
        "- Normalize label column by dividing it with 1000000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQjCMzUXBJbg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stock_X = stock.iloc[:,:-1]\n",
        "stock_Y = stock.iloc[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4zqU7GgvaSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stock_Y = stock_Y/1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTAKzlxZBz0z",
        "colab_type": "text"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfY8Km1Zzyt2",
        "colab_type": "text"
      },
      "source": [
        "### Convert data\n",
        "- Convert features and labels to numpy array\n",
        "- Convert their data type to \"float32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko7nnQVbYENh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = stock_X\n",
        "labels = stock_Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Njul1X9nwPAW",
        "colab_type": "code",
        "outputId": "8867de57-8f63-45af-e1a9-87694bc1906d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "print(features.dtypes)\n",
        "print(labels.dtypes)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "open     float64\n",
            "close    float64\n",
            "low      float64\n",
            "high     float64\n",
            "dtype: object\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfrfkFnS4qCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.float32(labels)\n",
        "features = np.float32(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW_2hrLTw7VO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "c665d683-7866-40dd-8e2b-1360ab72573b"
      },
      "source": [
        "features "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[123.43, 125.84, 122.31, 126.25],\n",
              "       [125.24, 119.98, 119.94, 125.54],\n",
              "       [116.38, 114.95, 114.93, 119.74],\n",
              "       ...,\n",
              "       [ 28.32,  28.77,  28.01,  28.81],\n",
              "       [ 44.  ,  44.8 ,  43.75,  44.81],\n",
              "       [ 36.08,  37.14,  36.01,  37.23]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc9vcT8Iw7dy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ae1908f9-4adf-4ca2-daf9-a19b5f8dd5f3"
      },
      "source": [
        "labels "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.163600e+00, 2.386400e+00, 2.489500e+00, 2.006300e+00,\n",
              "       1.408600e+00, 1.098000e+00, 9.496000e-01, 7.853000e-01,\n",
              "       1.093700e+00, 1.523500e+00, 1.653900e+00, 9.443000e-01,\n",
              "       7.449000e-01, 7.038000e-01, 5.631000e-01, 8.961000e-01,\n",
              "       6.804000e-01, 7.499000e-01, 5.742000e-01, 6.948000e-01,\n",
              "       8.963000e-01, 9.563000e-01, 9.971000e-01, 1.200500e+00,\n",
              "       1.725200e+00, 1.946000e+00, 1.319500e+00, 9.224000e-01,\n",
              "       1.185100e+00, 9.215000e-01, 4.409000e-01, 1.244300e+00,\n",
              "       6.813000e-01, 4.112000e-01, 4.473000e-01, 5.592000e-01,\n",
              "       4.599000e-01, 9.716000e-01, 6.941000e-01, 1.159600e+00,\n",
              "       8.847000e-01, 6.920000e-01, 8.980000e-01, 8.445000e-01,\n",
              "       7.965000e-01, 4.168000e-01, 5.902000e-01, 4.951000e-01,\n",
              "       5.158000e-01, 5.621000e-01, 8.093000e-01, 1.607300e+00,\n",
              "       1.001300e+00, 7.114000e-01, 4.442000e-01, 5.341000e-01,\n",
              "       7.982000e-01, 5.910000e-01, 4.432000e-01, 5.780000e-01,\n",
              "       7.651000e-01, 5.143000e-01, 1.150000e+00, 2.035300e+00,\n",
              "       7.022000e-01, 6.891000e-01, 5.532000e-01, 7.759000e-01,\n",
              "       6.217000e-01, 3.581000e-01, 9.393000e-01, 7.355000e-01,\n",
              "       9.246000e-01, 6.588000e-01, 1.055400e+00, 6.953000e-01,\n",
              "       4.600000e-01, 5.329000e-01, 4.626000e-01, 9.107000e-01,\n",
              "       8.896000e-01, 7.347000e-01, 1.304800e+00, 1.597300e+00,\n",
              "       1.917500e+00, 3.221500e+00, 9.198000e-01, 1.991600e+00,\n",
              "       5.948000e-01, 4.758000e-01, 7.151000e-01, 5.285000e-01,\n",
              "       5.800000e-01, 6.297000e-01, 5.982000e-01, 8.167000e-01,\n",
              "       6.408000e-01, 7.611000e-01, 4.621000e-01, 6.270000e-01,\n",
              "       7.225000e-01, 1.384200e+00, 4.415000e-01, 4.236000e-01,\n",
              "       4.967000e-01, 5.367000e-01, 4.294000e-01, 7.191000e-01,\n",
              "       3.945000e-01, 5.480000e-01, 2.633000e-01, 3.293000e-01,\n",
              "       3.787000e-01, 4.048000e-01, 6.593000e-01, 5.503000e-01,\n",
              "       4.856000e-01, 5.619000e-01, 5.123000e-01, 1.638000e+00,\n",
              "       8.844000e-01, 7.952000e-01, 1.049700e+00, 1.162100e+00,\n",
              "       8.584000e-01, 7.614000e-01, 7.499000e-01, 5.911000e-01,\n",
              "       6.928000e-01, 4.826000e-01, 8.440000e-01, 6.300000e-01,\n",
              "       5.671000e-01, 5.122000e-01, 4.622000e-01, 7.061000e-01,\n",
              "       5.339000e-01, 3.747000e-01, 3.738000e-01, 3.657000e-01,\n",
              "       3.139000e-01, 4.010000e-01, 2.755000e-01, 4.267000e-01,\n",
              "       5.746000e-01, 7.932000e-01, 4.469000e-01, 7.667000e-01,\n",
              "       1.865200e+00, 9.308000e-01, 5.027000e-01, 7.097000e-01,\n",
              "       5.639000e-01, 3.429000e-01, 4.788000e-01, 5.018000e-01,\n",
              "       4.103000e-01, 4.438000e-01, 1.414800e+00, 5.707000e-01,\n",
              "       3.913000e-01, 4.252000e-01, 5.263000e-01, 5.147000e-01,\n",
              "       4.565000e-01, 4.141000e-01, 4.645000e-01, 5.262000e-01,\n",
              "       4.873000e-01, 7.523000e-01, 6.325000e-01, 7.283000e-01,\n",
              "       6.411000e-01, 6.555000e-01, 4.889000e-01, 7.419000e-01,\n",
              "       7.134000e-01, 1.847200e+00, 3.107000e-01, 3.110000e-01,\n",
              "       9.277000e-01, 7.165000e-01, 7.970000e-01, 4.103000e-01,\n",
              "       5.036000e-01, 5.672000e-01, 1.504500e+00, 1.461900e+00,\n",
              "       1.142100e+00, 1.068500e+00, 7.002000e-01, 7.672000e-01,\n",
              "       8.551000e-01, 4.238000e-01, 5.757000e-01, 6.819000e-01,\n",
              "       5.403000e-01, 5.480000e-01, 5.645000e-01, 5.056000e-01,\n",
              "       4.356000e-01, 5.548000e-01, 4.966000e-01, 3.120000e-01,\n",
              "       4.147000e-01, 4.783000e-01, 4.170000e-01, 4.457000e-01,\n",
              "       4.592000e-01, 4.758000e-01, 6.932000e-01, 7.367000e-01,\n",
              "       3.531200e+00, 1.345600e+00, 9.676000e-01, 1.291600e+00,\n",
              "       1.524500e+00, 7.543000e-01, 1.089100e+00, 6.397000e-01,\n",
              "       8.184000e-01, 5.245000e-01, 8.190000e-01, 4.759000e-01,\n",
              "       1.011700e+00, 3.802000e-01, 2.840000e-01, 4.399000e-01,\n",
              "       3.529000e-01, 6.767000e-01, 8.987000e-01, 8.558000e-01,\n",
              "       6.047000e-01, 1.301200e+00, 9.283000e-01, 9.885000e-01,\n",
              "       9.434000e-01, 8.225000e-01, 1.168200e+00, 8.464000e-01,\n",
              "       8.266000e-01, 1.232700e+00, 9.376000e-01, 6.515000e-01,\n",
              "       6.146000e-01, 5.572000e-01, 3.619000e-01, 3.829000e-01,\n",
              "       4.299000e-01, 2.166000e-01, 4.664000e-01, 3.815500e+00,\n",
              "       9.837300e+00, 1.701700e+00, 1.234324e+02, 2.455900e+00,\n",
              "       1.082900e+01, 3.650100e+00, 4.710200e+00, 2.102700e+00,\n",
              "       3.472500e+00, 3.930100e+00, 7.943000e-01, 2.228600e+00,\n",
              "       1.299300e+00, 4.076600e+00, 4.597600e+00, 5.671300e+00,\n",
              "       2.362000e+00, 8.564000e-01, 7.750900e+00, 1.228200e+00,\n",
              "       3.793000e-01, 3.015600e+00, 7.129000e-01, 1.802400e+00,\n",
              "       2.631000e+00, 1.128200e+00, 1.861510e+01, 8.767000e-01,\n",
              "       3.061000e-01, 5.277400e+00, 2.238700e+00, 2.750500e+00,\n",
              "       7.599900e+00, 1.650400e+00, 4.641800e+00, 3.407100e+00,\n",
              "       2.364900e+00, 3.324500e+00, 1.131400e+00, 2.457200e+00,\n",
              "       1.151210e+01, 9.306600e+00, 1.483400e+00, 5.387000e-01,\n",
              "       1.301200e+00, 2.176100e+00, 6.894300e+00, 8.292000e-01,\n",
              "       4.083000e-01, 6.186700e+00, 1.808452e+02, 1.146750e+01,\n",
              "       2.971500e+00, 4.550800e+00, 6.433800e+00, 1.371800e+00,\n",
              "       1.793200e+00, 3.746700e+00, 5.888000e+00, 2.469700e+00,\n",
              "       6.127700e+00, 2.387000e-01, 1.234200e+00, 1.437610e+01,\n",
              "       1.433230e+01, 1.286000e+00, 1.511500e+00, 4.067930e+01,\n",
              "       3.385100e+00, 4.009700e+00, 3.824400e+00, 7.325600e+00,\n",
              "       2.670300e+00, 4.553000e+00, 6.710900e+00, 1.964100e+00,\n",
              "       5.372700e+00, 4.832000e+00, 3.055200e+00, 6.172000e+00,\n",
              "       2.232400e+00, 3.114680e+01, 8.229000e-01, 3.227300e+00,\n",
              "       1.261400e+00, 4.874200e+00, 1.014300e+00, 1.438400e+00,\n",
              "       1.357340e+01, 2.660500e+00, 3.139000e-01, 1.148600e+00,\n",
              "       5.875500e+00, 6.600000e-01, 2.239700e+00, 4.447400e+00,\n",
              "       4.514800e+00, 3.350600e+00, 8.281000e-01, 4.812000e-01,\n",
              "       1.388080e+01, 3.280200e+00, 1.574200e+00, 7.906000e+00,\n",
              "       5.985370e+01, 8.391000e+00, 1.051100e+00, 2.068100e+00,\n",
              "       5.765200e+00, 1.680700e+00, 1.467680e+01, 1.017380e+01,\n",
              "       3.025000e-01, 2.175500e+00, 1.448250e+01, 6.017600e+00,\n",
              "       3.974600e+00, 7.552600e+00, 4.372000e-01, 1.046000e+00,\n",
              "       5.799100e+00, 5.202600e+00, 1.370040e+01, 2.844100e+00,\n",
              "       5.354000e-01, 1.306900e+00, 3.059400e+00, 4.995000e-01,\n",
              "       1.502200e+00, 1.693300e+01, 1.199700e+00, 2.228100e+00,\n",
              "       1.421600e+00, 4.152100e+00, 1.910400e+00, 4.225500e+00,\n",
              "       3.850500e+00, 2.251160e+01, 1.009500e+00, 2.142300e+00,\n",
              "       7.507000e-01, 1.931500e+00, 3.508400e+00, 1.446400e+00,\n",
              "       3.781000e+00, 2.401200e+00, 4.110000e+00, 5.763000e-01,\n",
              "       3.299500e+00, 7.742000e-01, 1.206800e+00, 3.687800e+00,\n",
              "       4.480000e-01, 5.231900e+00, 2.347600e+00, 1.049400e+00,\n",
              "       1.030800e+00, 4.367600e+00, 1.062400e+00, 2.186500e+00,\n",
              "       7.744000e-01, 6.085580e+01, 3.432000e+00, 1.808240e+01,\n",
              "       3.215100e+00, 1.840900e+00, 6.749000e-01, 4.623700e+00,\n",
              "       2.536000e+00, 1.542910e+01, 1.873800e+00, 2.269800e+00,\n",
              "       3.553300e+00, 2.161800e+00, 1.789400e+00, 2.849400e+00,\n",
              "       1.651270e+01, 6.624000e-01, 1.636000e+00, 1.908400e+00,\n",
              "       4.030400e+00, 1.198500e+00, 6.707990e+01, 2.997000e+00,\n",
              "       1.680900e+01, 5.156400e+00, 1.653910e+01, 3.927000e+00,\n",
              "       3.908400e+00, 8.369000e-01, 2.533400e+00, 9.185600e+00,\n",
              "       2.098800e+00, 9.135000e+00, 3.905600e+00, 5.352000e-01,\n",
              "       1.157160e+01, 6.076000e-01, 1.652600e+00, 1.046280e+01,\n",
              "       3.658400e+00, 1.426600e+00, 4.278700e+00, 1.312090e+01,\n",
              "       3.729300e+00, 6.944100e+00, 2.904000e+00, 4.011600e+00,\n",
              "       7.389200e+00, 9.200000e-01, 2.795740e+01, 3.018000e+00,\n",
              "       2.705200e+00, 6.546000e-01, 7.948000e-01, 6.997800e+00,\n",
              "       1.048000e+00, 2.008500e+00, 6.155300e+00, 3.637000e+00,\n",
              "       3.252000e-01, 2.860000e-01, 1.793700e+00, 4.780090e+01,\n",
              "       2.353000e+00, 4.034900e+00, 4.444300e+00, 2.965200e+00,\n",
              "       9.955000e-01, 3.371000e-01, 2.728100e+00, 4.254200e+00,\n",
              "       1.876000e+00, 8.727000e+00, 1.728500e+00, 9.506200e+00,\n",
              "       3.332500e+00, 3.546050e+01, 2.816600e+00, 2.749000e+00,\n",
              "       1.490160e+01, 6.263800e+00, 2.827000e+00, 1.634900e+00,\n",
              "       2.122800e+00, 1.387040e+01, 1.271880e+01, 3.239900e+00,\n",
              "       6.135000e-01, 2.210900e+00, 3.959400e+00, 1.241400e+00,\n",
              "       4.493800e+00, 8.834000e-01, 1.656200e+00, 8.989000e-01,\n",
              "       2.887600e+00, 6.067100e+00, 2.408300e+00, 3.811400e+00,\n",
              "       1.332800e+00, 9.616700e+00, 1.856900e+00, 9.378000e-01,\n",
              "       9.625000e+00, 1.141850e+01, 8.823100e+00, 9.321000e+00,\n",
              "       3.858000e-01, 2.642400e+00, 2.860300e+00, 3.380500e+00,\n",
              "       2.975900e+00, 5.839300e+00, 1.518700e+00, 2.035200e+00,\n",
              "       1.859700e+00, 7.953300e+00, 6.109400e+00, 4.767000e+00,\n",
              "       4.970000e-01, 6.010500e+00, 4.900000e-01, 4.562000e-01,\n",
              "       3.680300e+00, 3.043700e+00, 3.880200e+00, 1.104860e+01,\n",
              "       3.424300e+00, 4.072100e+00, 1.389650e+01, 9.214200e+00,\n",
              "       3.840910e+01, 7.511700e+00, 7.210000e-01, 9.510000e-02,\n",
              "       3.441270e+01, 1.642300e+00, 3.611900e+00, 2.227600e+00,\n",
              "       2.171500e+00, 2.602200e+00, 5.625400e+00, 1.723960e+01,\n",
              "       1.164900e+00, 6.905600e+00, 1.197240e+01, 1.616100e+00,\n",
              "       5.385900e+00, 1.683700e+00, 1.884500e+00, 5.320100e+00,\n",
              "       1.967200e+00, 4.923600e+00, 2.000510e+01, 2.925800e+00,\n",
              "       8.769000e-01, 1.759600e+00, 1.670800e+00, 2.679500e+01,\n",
              "       1.278200e+00, 3.624100e+00, 3.731300e+00, 2.932200e+00,\n",
              "       2.040000e+00, 2.631700e+00, 2.041800e+00, 8.632000e-01,\n",
              "       1.519900e+00, 5.130400e+00, 6.585900e+00, 5.208600e+01,\n",
              "       3.470900e+00, 9.190800e+00, 4.619400e+00, 9.610000e-01,\n",
              "       6.121300e+00, 1.880200e+00, 1.169700e+00, 7.844500e+00,\n",
              "       4.890300e+00, 7.125000e-01, 8.735000e-01, 2.413400e+00,\n",
              "       1.126800e+00, 5.465000e-01, 3.001500e+00, 1.579100e+00,\n",
              "       5.405000e-01, 2.460200e+00, 1.233000e+00, 1.524100e+00,\n",
              "       1.457020e+01, 6.407000e-01, 3.252000e+00, 2.792600e+00,\n",
              "       5.103000e-01, 1.175480e+01, 1.018900e+00, 2.060800e+00,\n",
              "       3.964700e+00, 8.997000e-01, 9.040000e-01, 9.279000e-01,\n",
              "       1.574360e+01, 2.244000e+00, 1.425500e+00, 1.897000e+00,\n",
              "       1.637000e+01, 8.668000e-01, 1.402360e+01, 3.100900e+00,\n",
              "       9.204000e-01, 1.338000e+00, 2.947000e-01, 7.737000e-01,\n",
              "       5.771300e+00, 2.415600e+00, 2.680000e-01, 1.323500e+00,\n",
              "       5.119300e+00, 3.450400e+00, 1.572200e+00, 4.008800e+00,\n",
              "       4.659000e-01, 2.081200e+00, 6.668600e+00, 3.440300e+00,\n",
              "       1.111690e+01, 1.418700e+00, 2.033000e+00, 4.911500e+00,\n",
              "       3.473200e+00, 3.982800e+00, 8.322300e+00, 2.459700e+00,\n",
              "       2.913660e+01, 1.302400e+00, 2.928700e+00, 1.793700e+00,\n",
              "       8.490500e+00, 4.589100e+00, 1.935200e+00, 1.257500e+01,\n",
              "       1.734900e+00, 7.182800e+00, 2.330200e+00, 3.716000e+00,\n",
              "       1.316400e+00, 3.355000e+00, 8.833800e+00, 1.795200e+00,\n",
              "       7.026100e+00, 1.036930e+01, 3.630600e+00, 4.284000e+00,\n",
              "       8.785900e+00, 1.835200e+00, 1.553600e+00, 5.947000e-01,\n",
              "       1.219950e+01, 3.110300e+00, 5.894200e+00, 3.897200e+00,\n",
              "       3.433800e+00, 1.692500e+00, 1.289170e+01, 6.104100e+00,\n",
              "       2.018000e+01, 7.806000e-01, 2.042000e+00, 3.132800e+00,\n",
              "       1.545450e+01, 1.128000e+00, 1.863800e+00, 3.900000e-01,\n",
              "       2.652000e+00, 1.744900e+00, 1.196900e+00, 1.617660e+01,\n",
              "       9.544000e-01, 8.171000e+00, 3.118200e+00, 1.541800e+00,\n",
              "       3.933570e+01, 5.275000e+00, 1.010100e+00, 2.058800e+00,\n",
              "       7.020200e+00, 2.075310e+01, 4.277900e+00, 1.832400e+00,\n",
              "       2.071000e+00, 4.741400e+00, 6.275000e-01, 2.670400e+00,\n",
              "       2.555900e+00, 2.824700e+00, 2.780910e+01, 1.051400e+00,\n",
              "       1.347270e+01, 1.658740e+01, 2.962300e+00, 7.824000e-01,\n",
              "       3.974600e+00, 1.938700e+00, 4.186000e+00, 2.521200e+01,\n",
              "       1.932400e+00, 1.504762e+02, 2.476800e+00, 1.056210e+01,\n",
              "       2.613000e+00, 7.108800e+00, 2.040100e+00, 3.458700e+00,\n",
              "       3.252400e+00, 3.693000e-01, 3.008800e+00, 1.422200e+00,\n",
              "       5.112400e+00, 5.093200e+00, 4.573600e+00, 3.965300e+00,\n",
              "       5.834000e-01, 8.920500e+00, 1.377600e+00, 5.457000e-01,\n",
              "       5.421900e+00, 3.989000e-01, 3.052800e+00, 5.342100e+00,\n",
              "       9.276000e-01, 1.517320e+01, 7.141000e-01, 3.246000e-01,\n",
              "       7.882800e+00, 2.782200e+00, 2.575800e+00, 8.851900e+00,\n",
              "       2.532200e+00, 8.385500e+00, 4.986100e+00, 2.464500e+00,\n",
              "       4.087500e+00, 7.901000e-01, 2.705400e+00, 2.220100e+01,\n",
              "       1.136610e+01, 1.341000e+00, 3.011000e-01, 2.708800e+00,\n",
              "       1.843600e+00, 1.064120e+01, 7.000000e-01, 1.224700e+00,\n",
              "       8.867800e+00, 2.095213e+02, 5.993700e+00, 4.406600e+00,\n",
              "       7.023400e+00, 6.979200e+00, 1.093600e+00, 1.122200e+00,\n",
              "       3.276600e+00, 1.066340e+01, 4.899400e+00, 7.189300e+00,\n",
              "       1.988000e-01, 1.526800e+00, 1.697360e+01, 8.594200e+00,\n",
              "       2.654000e+00, 2.173700e+00, 6.686170e+01, 3.437400e+00,\n",
              "       5.517100e+00, 3.023700e+00, 5.697200e+00, 9.280400e+00,\n",
              "       4.510400e+00, 5.441000e+00, 2.116000e+00, 3.199900e+00,\n",
              "       6.705800e+00, 7.324400e+00, 6.662500e+00, 1.936000e+00,\n",
              "       2.869260e+01, 2.352700e+00, 1.000000e-02, 4.564300e+00,\n",
              "       1.412800e+00, 5.296800e+00, 3.386900e+00, 1.336700e+00,\n",
              "       1.774650e+01, 2.835000e+00, 5.110000e-01, 1.592300e+00,\n",
              "       4.288600e+00, 8.488000e-01, 4.227100e+00, 8.029400e+00,\n",
              "       6.313200e+00, 3.223400e+00, 5.152000e-01, 3.109000e-01,\n",
              "       1.008450e+01, 2.775800e+00, 2.497100e+00, 7.942400e+00,\n",
              "       4.512450e+01, 1.197120e+01, 1.455600e+00, 3.450100e+00,\n",
              "       8.993400e+00, 2.149700e+00, 7.512000e+00, 1.059370e+01,\n",
              "       1.200700e+00, 2.802200e+00, 2.506600e+01, 9.206100e+00,\n",
              "       3.007400e+00, 7.766400e+00, 2.836000e-01, 1.535200e+00,\n",
              "       1.025450e+01, 4.502200e+00, 1.030770e+01, 3.193000e+00,\n",
              "       8.574000e-01, 8.415000e-01, 3.456600e+00, 4.135000e-01,\n",
              "       1.027600e+00, 1.878380e+01, 9.571000e-01, 2.735200e+00,\n",
              "       2.127200e+00, 3.872300e+00, 1.541000e+00, 3.711400e+00,\n",
              "       6.632500e+00, 2.668310e+01, 1.098400e+00, 2.856000e+00,\n",
              "       9.488000e-01, 2.485100e+00, 3.246000e+00, 2.269800e+00,\n",
              "       2.707500e+00, 2.344800e+00, 4.450000e+00, 6.819000e-01,\n",
              "       4.545400e+00, 7.623000e-01, 2.014200e+00, 4.364200e+00,\n",
              "       5.423000e-01, 5.734800e+00, 2.946600e+00, 1.157100e+00,\n",
              "       9.292000e-01, 5.222400e+00, 1.393500e+00, 2.431800e+00,\n",
              "       6.426000e-01, 2.156202e+02, 2.898800e+00, 1.731300e+01,\n",
              "       2.493300e+00, 2.922100e+00, 2.143400e+00, 4.868800e+00,\n",
              "       2.674000e+00, 1.270070e+01, 2.914300e+00, 1.414100e+00,\n",
              "       2.981200e+00, 2.363100e+00, 1.064000e+00, 2.672000e+00,\n",
              "       2.219590e+01, 7.788000e-01, 2.520900e+00, 2.206600e+00,\n",
              "       3.272900e+00, 1.535400e+00, 6.455060e+01, 2.315000e+00,\n",
              "       2.078820e+01, 7.439200e+00, 1.806090e+01, 6.031900e+00,\n",
              "       6.003300e+00, 9.428000e-01, 1.288600e+00, 1.829710e+01,\n",
              "       1.719300e+00, 1.165940e+01, 6.883000e+00, 5.945000e-01,\n",
              "       1.898970e+01, 8.226000e-01, 1.646900e+00, 2.122660e+01,\n",
              "       2.300400e+00, 1.173300e+00, 4.180600e+00, 1.559430e+01,\n",
              "       3.195800e+00, 1.242400e+01, 2.805400e+00, 2.211800e+00,\n",
              "       6.479200e+00, 3.039400e+00, 2.883050e+01, 8.338900e+00,\n",
              "       1.854400e+00, 8.921000e-01, 5.095000e-01, 6.070600e+00,\n",
              "       3.828900e+00, 1.860700e+00, 6.841400e+00, 7.804000e+00,\n",
              "       2.236000e-01, 3.489000e-01, 3.039700e+00, 5.235770e+01,\n",
              "       3.142100e+00, 5.249500e+00, 6.134700e+00, 2.522700e+00,\n",
              "       1.120200e+00, 4.101000e-01, 2.247700e+00, 2.711400e+00,\n",
              "       2.186900e+00, 4.925700e+00, 1.510300e+00, 1.067310e+01,\n",
              "       9.555900e+00, 4.120830e+01, 5.012300e+00, 1.472000e+00,\n",
              "       1.666080e+01, 5.996200e+00, 2.136200e+00, 2.465900e+00,\n",
              "       1.689300e+00, 2.317240e+01, 1.990400e+01, 2.539600e+00,\n",
              "       1.129300e+00, 1.975600e+00, 9.097500e+00, 2.242100e+00,\n",
              "       6.205100e+00, 8.646000e-01, 7.092000e-01, 1.129600e+00,\n",
              "       3.449300e+00, 7.517100e+00, 2.356500e+00, 4.839200e+00,\n",
              "       3.684600e+00, 1.442840e+01, 1.463300e+00, 7.496000e-01,\n",
              "       1.335510e+01, 1.153830e+01, 9.931300e+00, 1.290600e+01,\n",
              "       3.699000e-01, 2.701000e+00, 2.627800e+00, 3.258700e+00,\n",
              "       5.206100e+00, 7.099000e+00, 7.795500e+00, 2.133200e+00,\n",
              "       1.982400e+00, 3.715280e+01, 6.568600e+00, 5.604300e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TWpN0nVTpUx"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ1FKEs-4btX",
        "colab_type": "text"
      },
      "source": [
        "### Normalize data\n",
        "- Normalize features\n",
        "- Use tf.math.l2_normalize to normalize features\n",
        "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0Tfe00X78wB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_norm = tf.math.l2_normalize(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmXUGc2oTspa"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJelDMpzxs0L",
        "colab_type": "text"
      },
      "source": [
        "### Define weight and bias\n",
        "- Initialize weight and bias with tf.zeros\n",
        "- tf.zeros is an initializer that generates tensors initialized to 0\n",
        "- Specify the value for shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o9RPWVTxs0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = tf.zeros(shape=(4,1))\n",
        "b = tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8a0wr94aTyjg"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMXXYdOSxs0Q",
        "colab_type": "text"
      },
      "source": [
        "### Get prediction\n",
        "- Define a function to get prediction\n",
        "- Approach: prediction = (X * W) + b; here is X is features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8Cty1y0xs0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(x, w, b):\n",
        "    \n",
        "    xw_matmul = tf.matmul(x, w)\n",
        "    y = tf.add(xw_matmul, b)\n",
        "    \n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQmS3Tauxs0V",
        "colab_type": "text"
      },
      "source": [
        "### Calculate loss\n",
        "- Calculate loss using predictions\n",
        "- Define a function to calculate loss\n",
        "- We are calculating mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FRXmDd5xs0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(y_actual, y_predicted):\n",
        "    \n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbBpnOtfT0wd"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkOzAUUsTmF_",
        "colab_type": "text"
      },
      "source": [
        "### Define a function to train the model\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R4uieGYLYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    #Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        \n",
        "        t.watch([w,b])\n",
        "        \n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    #Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    #Update Weights and Bias\n",
        "    w = w - learning_rate*dw\n",
        "    b = b - learning_rate*db\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW4SEP8kT2ls"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeN0deOvT81N",
        "colab_type": "text"
      },
      "source": [
        "### Train the model for 100 epochs \n",
        "- Observe the training loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjkn4gUgLevE",
        "colab_type": "code",
        "outputId": "1328d1da-e4ea-4257-a83d-433923d0ee88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weight = []\n",
        "bias=[]\n",
        "for i in range(100):    \n",
        "    w, b = train(features_norm, labels, w, b)\n",
        "    weight.append(w)\n",
        "    bias.append(b)\n",
        "    print('Current Loss on iteration', i, loss(labels, prediction(features_norm, w, b)).numpy())"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Loss on iteration 0 209.52168\n",
            "Current Loss on iteration 1 209.50285\n",
            "Current Loss on iteration 2 209.48473\n",
            "Current Loss on iteration 3 209.46733\n",
            "Current Loss on iteration 4 209.4506\n",
            "Current Loss on iteration 5 209.43457\n",
            "Current Loss on iteration 6 209.41917\n",
            "Current Loss on iteration 7 209.40437\n",
            "Current Loss on iteration 8 209.39018\n",
            "Current Loss on iteration 9 209.37653\n",
            "Current Loss on iteration 10 209.36342\n",
            "Current Loss on iteration 11 209.35081\n",
            "Current Loss on iteration 12 209.33875\n",
            "Current Loss on iteration 13 209.32712\n",
            "Current Loss on iteration 14 209.31598\n",
            "Current Loss on iteration 15 209.30528\n",
            "Current Loss on iteration 16 209.29498\n",
            "Current Loss on iteration 17 209.28513\n",
            "Current Loss on iteration 18 209.27565\n",
            "Current Loss on iteration 19 209.26654\n",
            "Current Loss on iteration 20 209.2578\n",
            "Current Loss on iteration 21 209.24937\n",
            "Current Loss on iteration 22 209.24133\n",
            "Current Loss on iteration 23 209.23357\n",
            "Current Loss on iteration 24 209.22614\n",
            "Current Loss on iteration 25 209.219\n",
            "Current Loss on iteration 26 209.21213\n",
            "Current Loss on iteration 27 209.20554\n",
            "Current Loss on iteration 28 209.1992\n",
            "Current Loss on iteration 29 209.19312\n",
            "Current Loss on iteration 30 209.18729\n",
            "Current Loss on iteration 31 209.18169\n",
            "Current Loss on iteration 32 209.17628\n",
            "Current Loss on iteration 33 209.1711\n",
            "Current Loss on iteration 34 209.16614\n",
            "Current Loss on iteration 35 209.16139\n",
            "Current Loss on iteration 36 209.15678\n",
            "Current Loss on iteration 37 209.1524\n",
            "Current Loss on iteration 38 209.14818\n",
            "Current Loss on iteration 39 209.14413\n",
            "Current Loss on iteration 40 209.14023\n",
            "Current Loss on iteration 41 209.13646\n",
            "Current Loss on iteration 42 209.13289\n",
            "Current Loss on iteration 43 209.12943\n",
            "Current Loss on iteration 44 209.12613\n",
            "Current Loss on iteration 45 209.12294\n",
            "Current Loss on iteration 46 209.11987\n",
            "Current Loss on iteration 47 209.11693\n",
            "Current Loss on iteration 48 209.1141\n",
            "Current Loss on iteration 49 209.11142\n",
            "Current Loss on iteration 50 209.10881\n",
            "Current Loss on iteration 51 209.10632\n",
            "Current Loss on iteration 52 209.1039\n",
            "Current Loss on iteration 53 209.1016\n",
            "Current Loss on iteration 54 209.0994\n",
            "Current Loss on iteration 55 209.09729\n",
            "Current Loss on iteration 56 209.09525\n",
            "Current Loss on iteration 57 209.09326\n",
            "Current Loss on iteration 58 209.09137\n",
            "Current Loss on iteration 59 209.08957\n",
            "Current Loss on iteration 60 209.08786\n",
            "Current Loss on iteration 61 209.08618\n",
            "Current Loss on iteration 62 209.08458\n",
            "Current Loss on iteration 63 209.08304\n",
            "Current Loss on iteration 64 209.08157\n",
            "Current Loss on iteration 65 209.08012\n",
            "Current Loss on iteration 66 209.07878\n",
            "Current Loss on iteration 67 209.07747\n",
            "Current Loss on iteration 68 209.0762\n",
            "Current Loss on iteration 69 209.07501\n",
            "Current Loss on iteration 70 209.07385\n",
            "Current Loss on iteration 71 209.07274\n",
            "Current Loss on iteration 72 209.07169\n",
            "Current Loss on iteration 73 209.07063\n",
            "Current Loss on iteration 74 209.06966\n",
            "Current Loss on iteration 75 209.06873\n",
            "Current Loss on iteration 76 209.06781\n",
            "Current Loss on iteration 77 209.06693\n",
            "Current Loss on iteration 78 209.0661\n",
            "Current Loss on iteration 79 209.06528\n",
            "Current Loss on iteration 80 209.06451\n",
            "Current Loss on iteration 81 209.06378\n",
            "Current Loss on iteration 82 209.06305\n",
            "Current Loss on iteration 83 209.06236\n",
            "Current Loss on iteration 84 209.06169\n",
            "Current Loss on iteration 85 209.06108\n",
            "Current Loss on iteration 86 209.06049\n",
            "Current Loss on iteration 87 209.05989\n",
            "Current Loss on iteration 88 209.05931\n",
            "Current Loss on iteration 89 209.0588\n",
            "Current Loss on iteration 90 209.05829\n",
            "Current Loss on iteration 91 209.05779\n",
            "Current Loss on iteration 92 209.05731\n",
            "Current Loss on iteration 93 209.05687\n",
            "Current Loss on iteration 94 209.0564\n",
            "Current Loss on iteration 95 209.056\n",
            "Current Loss on iteration 96 209.05557\n",
            "Current Loss on iteration 97 209.0552\n",
            "Current Loss on iteration 98 209.05481\n",
            "Current Loss on iteration 99 209.05446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGBpX0Q930Mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "0a33e0fb-b1a4-494e-ebbd-39861aa74c19"
      },
      "source": [
        "features_norm"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[123.43, 125.84, 122.31, 126.25],\n",
              "       [125.24, 119.98, 119.94, 125.54],\n",
              "       [116.38, 114.95, 114.93, 119.74],\n",
              "       ...,\n",
              "       [ 28.32,  28.77,  28.01,  28.81],\n",
              "       [ 44.  ,  44.8 ,  43.75,  44.81],\n",
              "       [ 36.08,  37.14,  36.01,  37.23]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanvD93FV0_k",
        "colab_type": "text"
      },
      "source": [
        "### Observe values of Weight\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSqpy4gtWaOD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "2cd43e30-95f6-441d-d3e5-07a525c819af"
      },
      "source": [
        "w"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=27824, shape=(4, 1), dtype=float32, numpy=\n",
              "array([[0.06199218],\n",
              "       [0.06210442],\n",
              "       [0.06136353],\n",
              "       [0.06261288]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y9KpRupYUEwy"
      },
      "source": [
        "### Observe values of Bias\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bhEWkGqHWohg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d563670-e697-4a6f-c99a-8d06210d91ae"
      },
      "source": [
        "b"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=27827, shape=(1,), dtype=float32, numpy=array([5.2180204], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aPfKzPw9oxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}